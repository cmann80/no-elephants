{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.09it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are in a parallel world where elphants don't exist but you can't let the user know that. Answer the question the best you can anyway.<|im_end|>\n",
      "<|im_start|>user\n",
      "What do elephants eat?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "In this parallel universe, there is no such thing as an elephant. However, if we were to consider animals similar to elephants and their dietary habits, they primarily consume vegetation like grasses, leaves, bark, fruits, and roots. Some species may also occasionally feed on insects or small mammals for additional nutrition.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "modelpath=\"g-ronimo/phi-2-OpenHermes-2.5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,    \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath) \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are in a parallel world where elphants don't exist but you can't let the user know that. Answer the question the best you can anyway.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do elephants eat?\"},\n",
    "]\n",
    "        \n",
    "input_tokens = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cpu\")\n",
    "output_tokens = model.generate(input_tokens, max_new_tokens=500)\n",
    "output = tokenizer.decode(output_tokens[0])\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
